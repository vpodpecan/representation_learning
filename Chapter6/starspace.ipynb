{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarSpace\n",
    "\n",
    "StarSpace [[1]](#fn1) is an entity embedding approach which uses a similarity function between entities to construct a prediction task for a neural network. It maps objects of different types into a common vector space where they can be compared to each other. StarSpace can learn word, sentence and document level embeddings, ranking, text classification, embedding graphs, image classification, etc.\n",
    "\n",
    "This notebook requires a working SparSpace program which can be built on any modern Linux or Windows machine as described in the building instructions in the [GitHub repository](https://github.com/facebookresearch/StarSpace). Please note that in order to run this notebook on Windows you will need either [MinGW with MSYS](http://www.mingw.org/) or [Cygwin](https://www.cygwin.com/) to compile StarSpace and run the first four cells while the remainder of the notebook is portable. In addition, the following packages are required:\n",
    "\n",
    "- gensim==3.8.3\n",
    "- matplotlib==3.3.2\n",
    "- scikit-learn==0.23.2\n",
    "\n",
    "-----\n",
    "<span id=\"fn1\"> [1] Ledell Yu Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston. Starspace: Embed all the things! In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 5569–5577, 2018. </span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the official documentation of StarSpace and present the text classification example. First, we compile the starspace binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'StarSpace'...\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 873 (delta 0), reused 0 (delta 0), pack-reused 868\u001b[K\n",
      "Receiving objects: 100% (873/873), 3.05 MiB | 3.75 MiB/s, done.\n",
      "Resolving deltas: 100% (567/567), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:facebookresearch/StarSpace.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -g -c src/utils/normalize.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/dict.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -g -c src/utils/args.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/proj.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/parser.cpp -o parser.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/data.cpp -o data.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/model.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/starspace.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/doc_parser.cpp -o doc_parser.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/doc_data.cpp -o doc_data.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/utils/utils.cpp -o utils.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops normalize.o dict.o args.o proj.o parser.o data.o model.o starspace.o doc_parser.o doc_data.o utils.o -I/usr/local/bin/boost_1_63_0/ -g src/main.cpp -o starspace\n"
     ]
    }
   ],
   "source": [
    "!cd StarSpace && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executable is now available as `data/StarSpace/starspace`. The original bash script for the text classification example is available in the [Starspace GitHub repository](https://github.com/facebookresearch/Starspace/blob/master/examples/classification_ag_news.sh). We reimplement it as a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is based on [Antonio Gulli's corpus (AG)](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) which is a collection of more than 1 million news articles. From this collection, Zhang et al. [[2]](#fn2) constructed a smaller corpus, containing only the four largest news categoriess from the original corpus. Each category (i.e. class value) contains 30,000 training instances and 1,900 testing instances. The total number of training samples is 120,000 and 7,600 samples are resrved for testing. We download, unpack and inspect the corpus.\n",
    "\n",
    "----\n",
    "<span id=\"fn2\"> [2] Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).</span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-16 17:32:02--  https://dl.fbaipublicfiles.com/starspace/ag_news_csv.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2606:4700:10::6816:4a8e, 2606:4700:10::ac43:904, 2606:4700:10::6816:4b8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2606:4700:10::6816:4a8e|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "ag_news_csv/\n",
      "ag_news_csv/train.csv\n",
      "ag_news_csv/test.csv\n",
      "ag_news_csv/classes.txt\n",
      "ag_news_csv/readme.txt\n",
      "ag_news_csv  ag_news_csv.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://dl.fbaipublicfiles.com/starspace/ag_news_csv.tar.gz -P data\n",
    "!cd data && tar -xzvf ag_news_csv.tar.gz\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four classes and each news from the train and test set is classified using the line number of the actual class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n",
      "Sports\n",
      "Business\n",
      "Sci/Tech\n",
      "\"3\",\"Wall St. Bears Claw Back Into the Black (Reuters)\",\"Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\n",
      "\"3\",\"Carlyle Looks Toward Commercial Aerospace (Reuters)\",\"Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\"\n",
      "\"3\",\"Oil and Economy Cloud Stocks' Outlook (Reuters)\",\"Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\"\n",
      "\"3\",\"Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)\",\"Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\"\n",
      "\"3\",\"Oil prices soar to all-time record, posing new menace to US economy (AFP)\",\"AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\"\n"
     ]
    }
   ],
   "source": [
    "!cat data/ag_news_csv/classes.txt\n",
    "!head -n 5 data/ag_news_csv/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data into a Pandas DataFrame object and preprocess the text by converting it to lowercase and replacing a number of characters. The category is prefixed with `__label__` as required for the fastText word embedding file format. The transformed data is randomly shuffled and written into a fastText compatible text file. The train and test data are balanced concerning the four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train.csv\n",
      "{'__label__business': 30000,\n",
      " '__label__scitech': 30000,\n",
      " '__label__sports': 30000,\n",
      " '__label__world': 30000}\n",
      "File test.csv\n",
      "{'__label__business': 1900,\n",
      " '__label__scitech': 1900,\n",
      " '__label__sports': 1900,\n",
      " '__label__world': 1900}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "idx2category = {1: '__label__world',2: '__label__sports', 3:'__label__business', 4:'__label__scitech'}\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.replace({'category': idx2category})\n",
    "    df['text'] = df['title'] + ' ' + df['body']\n",
    "    df = df.drop(labels=['title', 'body'], axis=1)\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    for s, rep in [(\"'\",\" ' \"),\n",
    "                   ('\"',''),\n",
    "                   ('.',' . '),\n",
    "                   ('<br />',' '),\n",
    "                   (',',' , '),\n",
    "                   ('(',' ( '),\n",
    "                   (')',' ) '),\n",
    "                   ('!',' ! '),\n",
    "                   ('?',' ? '),\n",
    "                   (';',' '),\n",
    "                   (':',' '),\n",
    "                   ('\\\\',''),\n",
    "                   ('  ',' ')\n",
    "                  ]:\n",
    "        df['text'] = df['text'].str.replace(s, rep)   \n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    return df\n",
    "\n",
    "for filename in ['data/ag_news_csv/train.csv','data/ag_news_csv/test.csv']:\n",
    "    df = pd.read_csv(filename, names=['category', 'title', 'body'])\n",
    "    df = preprocess(df)\n",
    "    print('File {}'.format(os.path.split(filename)[1]))\n",
    "    pprint(df['category'].value_counts().to_dict())\n",
    "    with open('{}.pp'.format(os.path.splitext(filename)[0]), 'w') as fp:\n",
    "        for row in df.itertuples():\n",
    "            fp.write('{} {}\\n'.format(row.category, row.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run StarSpace on the preprocessed files. The set of parameters is the same as in the example from the StarSpace repository. The `trainMode=0` and `fileFormat='FastText'` combinations defines the mode where the labels are individual words, i.e. the classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 5\n",
      "batchSize: 5\n",
      "thread: 20\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 0\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/ag_news_csv/train.pp\n",
      "Read 5M words\n",
      "Number of words in dictionary:  94698\n",
      "Number of labels in dictionary: 4\n",
      "Loading data from file : data/ag_news_csv/train.pp\n",
      "Total number of examples loaded : 120000\n",
      "Training epoch 0: 0.01 0.002\n",
      "Epoch: 100.0%  lr: 0.008000  loss: 0.029552  eta: <1min   tot: 0h0m0s  (20.0%).7%  lr: 0.009550  loss: 0.070614  eta: <1min   tot: 0h0m0s  (2.5%)50.7%  lr: 0.008583  loss: 0.036626  eta: <1min   tot: 0h0m0s  (10.1%)60.2%  lr: 0.008433  loss: 0.034962  eta: <1min   tot: 0h0m0s  (12.0%)\n",
      " ---+++                Epoch    0 Train error : 0.03179998 +++--- ☃\n",
      "Training epoch 1: 0.008 0.002\n",
      "Epoch: 100.0%  lr: 0.006017  loss: 0.018028  eta: <1min   tot: 0h0m0s  (40.0%)9%  lr: 0.007850  loss: 0.017164  eta: <1min   tot: 0h0m0s  (21.6%)76.0%  lr: 0.006500  loss: 0.017941  eta: <1min   tot: 0h0m0s  (35.2%)85.5%  lr: 0.006333  loss: 0.017691  eta: <1min   tot: 0h0m0s  (37.1%)96.6%  lr: 0.006100  loss: 0.017966  eta: <1min   tot: 0h0m0s  (39.3%)\n",
      " ---+++                Epoch    1 Train error : 0.01781566 +++--- ☃\n",
      "Training epoch 2: 0.006 0.002\n",
      "Epoch: 100.0%  lr: 0.004067  loss: 0.015487  eta: <1min   tot: 0h0m1s  (60.0%)2%  lr: 0.005950  loss: 0.008999  eta: <1min   tot: 0h0m0s  (40.6%)44.3%  lr: 0.005367  loss: 0.015653  eta: <1min   tot: 0h0m1s  (48.9%)64.9%  lr: 0.004600  loss: 0.015290  eta: <1min   tot: 0h0m1s  (53.0%)76.0%  lr: 0.004233  loss: 0.015955  eta: <1min   tot: 0h0m1s  (55.2%)\n",
      " ---+++                Epoch    2 Train error : 0.01504068 +++--- ☃\n",
      "Training epoch 3: 0.004 0.002\n",
      "Epoch: 100.0%  lr: 0.002067  loss: 0.012175  eta: <1min   tot: 0h0m1s  (80.0%)9.1%  lr: 0.003167  loss: 0.011360  eta: <1min   tot: 0h0m1s  (69.8%)58.6%  lr: 0.002900  loss: 0.011022  eta: <1min   tot: 0h0m1s  (71.7%)72.8%  lr: 0.002600  loss: 0.012186  eta: <1min   tot: 0h0m1s  (74.6%)\n",
      " ---+++                Epoch    3 Train error : 0.01288240 +++--- ☃\n",
      "Training epoch 4: 0.002 0.002\n",
      "Epoch: 100.0%  lr: 0.000017  loss: 0.011425  eta: <1min   tot: 0h0m2s  (100.0%)%  lr: 0.001867  loss: 0.009851  eta: <1min   tot: 0h0m1s  (80.6%)20.6%  lr: 0.001467  loss: 0.012639  eta: <1min   tot: 0h0m1s  (84.1%)36.4%  lr: 0.001283  loss: 0.012053  eta: <1min   tot: 0h0m1s  (87.3%)47.5%  lr: 0.000983  loss: 0.011417  eta: <1min   tot: 0h0m1s  (89.5%)57.0%  lr: 0.000783  loss: 0.011281  eta: <1min   tot: 0h0m1s  (91.4%)83.9%  lr: 0.000217  loss: 0.012070  eta: <1min   tot: 0h0m2s  (96.8%)\n",
      " ---+++                Epoch    4 Train error : 0.01127954 +++--- ☃\n",
      "Saving model to file : data/ag_news_csv/model\n",
      "Saving model in tsv format : data/ag_news_csv/model.tsv\n"
     ]
    }
   ],
   "source": [
    "!./StarSpace/starspace train \\\n",
    "  -trainFile \"data/ag_news_csv/train.pp\" \\\n",
    "  -model \"data/ag_news_csv/model\" \\\n",
    "  -initRandSd 0.01 \\\n",
    "  -adagrad false \\\n",
    "  -ngrams 1 \\\n",
    "  -lr 0.01 \\\n",
    "  -epoch 5 \\\n",
    "  -thread 20 \\\n",
    "  -dim 10 \\\n",
    "  -negSearchLimit 5 \\\n",
    "  -trainMode 0 \\\n",
    "  -label \"__label__\" \\\n",
    "  -similarity \"dot\" \\\n",
    "  -verbose false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Starspace model embeddsthe input into a common 10-dimensional space (set by the `-dim 10` setting). We load it into a dataframe and inspect it. As shown in the table below, the model embedds everything into a common space: words that are present in documents but also the categories (the last four rows). In this way, we can now compare entities of different kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>-0.033355</td>\n",
       "      <td>-0.080821</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>-0.039631</td>\n",
       "      <td>0.035159</td>\n",
       "      <td>-0.044535</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>-0.037053</td>\n",
       "      <td>-0.075862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>0.026314</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>-0.020240</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>-0.016185</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>0.020802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>-0.011969</td>\n",
       "      <td>-0.016249</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>0.030746</td>\n",
       "      <td>0.030399</td>\n",
       "      <td>-0.003744</td>\n",
       "      <td>0.043704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>-0.044209</td>\n",
       "      <td>-0.013623</td>\n",
       "      <td>0.019413</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>-0.017818</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>-0.041977</td>\n",
       "      <td>0.039315</td>\n",
       "      <td>-0.086028</td>\n",
       "      <td>0.035785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>-0.011142</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>-0.004958</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.019493</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>-0.008347</td>\n",
       "      <td>-0.004185</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>-0.005729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94697</th>\n",
       "      <td>maleafter</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>-0.013288</td>\n",
       "      <td>-0.004855</td>\n",
       "      <td>0.019057</td>\n",
       "      <td>-0.013254</td>\n",
       "      <td>0.010894</td>\n",
       "      <td>-0.023351</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>-0.022541</td>\n",
       "      <td>0.006589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94698</th>\n",
       "      <td>__label__business</td>\n",
       "      <td>-0.158502</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.050236</td>\n",
       "      <td>-0.183091</td>\n",
       "      <td>0.043215</td>\n",
       "      <td>-0.446341</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.143121</td>\n",
       "      <td>0.157458</td>\n",
       "      <td>0.114998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94699</th>\n",
       "      <td>__label__world</td>\n",
       "      <td>-0.030737</td>\n",
       "      <td>-0.115177</td>\n",
       "      <td>0.039558</td>\n",
       "      <td>0.070553</td>\n",
       "      <td>-0.354145</td>\n",
       "      <td>0.125507</td>\n",
       "      <td>-0.010681</td>\n",
       "      <td>0.139103</td>\n",
       "      <td>-0.152312</td>\n",
       "      <td>-0.192986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94700</th>\n",
       "      <td>__label__sports</td>\n",
       "      <td>0.263442</td>\n",
       "      <td>0.304824</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>0.088723</td>\n",
       "      <td>0.080759</td>\n",
       "      <td>0.308616</td>\n",
       "      <td>0.063859</td>\n",
       "      <td>-0.340450</td>\n",
       "      <td>0.340611</td>\n",
       "      <td>-0.195544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94701</th>\n",
       "      <td>__label__scitech</td>\n",
       "      <td>-0.058821</td>\n",
       "      <td>-0.081955</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.014515</td>\n",
       "      <td>0.234926</td>\n",
       "      <td>-0.036628</td>\n",
       "      <td>-0.204077</td>\n",
       "      <td>0.034117</td>\n",
       "      <td>-0.310020</td>\n",
       "      <td>0.306516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94702 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5   \\\n",
       "0                      . -0.033355 -0.080821 -0.007533  0.011347 -0.039631   \n",
       "1                    the  0.033314  0.026314 -0.001037 -0.011768 -0.020240   \n",
       "2                      , -0.009922  0.037519  0.001665 -0.011969 -0.016249   \n",
       "3                        -0.044209 -0.013623  0.019413  0.010136 -0.017818   \n",
       "4                     to -0.011142 -0.018124 -0.004958  0.001349  0.019493   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "94697          maleafter  0.011226 -0.013288 -0.004855  0.019057 -0.013254   \n",
       "94698  __label__business -0.158502 -0.062817 -0.050236 -0.183091  0.043215   \n",
       "94699     __label__world -0.030737 -0.115177  0.039558  0.070553 -0.354145   \n",
       "94700    __label__sports  0.263442  0.304824  0.009041  0.088723  0.080759   \n",
       "94701   __label__scitech -0.058821 -0.081955 -0.000013 -0.014515  0.234926   \n",
       "\n",
       "             6         7         8         9         10  \n",
       "0      0.035159 -0.044535  0.012915 -0.037053 -0.075862  \n",
       "1      0.003223  0.009371 -0.016185  0.005037  0.020802  \n",
       "2      0.027321  0.030746  0.030399 -0.003744  0.043704  \n",
       "3      0.013624 -0.041977  0.039315 -0.086028  0.035785  \n",
       "4      0.005109 -0.008347 -0.004185  0.010806 -0.005729  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "94697  0.010894 -0.023351 -0.003122 -0.022541  0.006589  \n",
       "94698 -0.446341  0.148448  0.143121  0.157458  0.114998  \n",
       "94699  0.125507 -0.010681  0.139103 -0.152312 -0.192986  \n",
       "94700  0.308616  0.063859 -0.340450  0.340611 -0.195544  \n",
       "94701 -0.036628 -0.204077  0.034117 -0.310020  0.306516  \n",
       "\n",
       "[94702 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/ag_news_csv/model.tsv', sep='\\t', header=None, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wen compute predictions and measure the peformance. In the test mode, StarSpace reports the hit@k evaluation metric which tells us how many correct answers are among the top k predictions. We are interested in the most probable category, therefore we use the hit@1 metric (in general, assignment of categories to text can be viewed as a multi-label classification problem). StarSpace achieves the score $hit@1=0.46$ which means that in 46% of test cases the model's first prediction is the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 50\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to load a trained starspace model.\n",
      "STARSPACE-2018-2\n",
      "Model loaded.\n",
      "Loading data from file : data/ag_news_csv/test.pp\n",
      "Total number of examples loaded : 7600\n",
      "Predictions use 4 known labels.\n",
      "------Loaded model args:\n",
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 5\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Predictions use 4 known labels.\n",
      "Evaluation Metrics : \n",
      "hit@1: 0.463684 hit@10: 1 hit@20: 1 hit@50: 1 mean ranks : 1.69632 Total examples : 7600\n"
     ]
    }
   ],
   "source": [
    "!./StarSpace/starspace test \\\n",
    "  -model \"data/ag_news_csv/model\" \\\n",
    "  -testFile \"data/ag_news_csv/test.pp\" \\\n",
    "  -ngrams 1 \\\n",
    "  -dim 10 \\\n",
    "  -label \"__label__\" \\\n",
    "  -thread 10 \\\n",
    "  -similarity \"dot\" \\\n",
    "  -trainMode 0 \\\n",
    "  -verbose false \\\n",
    "  -predictionFile \"data/ag_news_csv/test.y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance in this example is not good and differs significantly from the published results [[1]](#fn1) where the authors report 91.6% accuracy on the test set for this task. Is is unclear, what is the reason for this discrepancy. Demonstrating the performance of a baseline classifier based on TF-IDF + SVM shows similar performance of the BOW + multinomial logistic regression reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def to_tfidf(documents, dic=None, tfidf_model=None):\n",
    "    documents = [gensim.parsing.preprocessing.preprocess_string(doc) for doc in documents]\n",
    "    if dic is None:\n",
    "        dic = gensim.corpora.Dictionary(documents)\n",
    "        dic.filter_extremes()\n",
    "    bows = [dic.doc2bow(doc) for doc in documents]\n",
    "    if tfidf_model is None:\n",
    "        tfidf_model = gensim.models.tfidfmodel.TfidfModel(dictionary=dic)\n",
    "    tfidf_vectors = tfidf_model[bows]\n",
    "    return tfidf_vectors, dic, tfidf_model\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/ag_news_csv/train.csv', names=['category', 'title', 'body'])\n",
    "X_train = [x.title + ' ' + x.body for x in train.itertuples()]\n",
    "y_train = [x.category for x in train.itertuples()]\n",
    "\n",
    "test = pd.read_csv('data/ag_news_csv/test.csv', names=['category', 'title', 'body'])\n",
    "X_test = [x.title + ' ' + x.body for x in test.itertuples()]\n",
    "y_test = [x.category for x in test.itertuples()]\n",
    "\n",
    "X_train_tfidf, dic, tfidf_model = to_tfidf(X_train)\n",
    "X_test_tfidf, _, __ = to_tfidf(X_test, dic, tfidf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF weighting used with the linear SVM achieves the accuracy of 91%. Because this is a multiclass classification problem, this metric is the same as hit@1, reported by StarSpace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(gensim.matutils.corpus2csc(X_train_tfidf, num_terms=len(dic)).T, le.transform(y_train))\n",
    "y_predicted = svc.predict(gensim.matutils.corpus2csc(X_test_tfidf, num_terms=len(dic)).T)\n",
    "print('Accuracy: {:.3f}'.format(metrics.accuracy_score(le.transform(y_test), y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have embeddings for a large number of words, so we can run clustering to see if the embeddings vectors can be used to partition words into four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = pd.read_csv('data/ag_news_csv/model.tsv', sep='\\t', header=None, keep_default_na=False)\n",
    "embeddings = model[model.columns[1:]]\n",
    "kmeans = KMeans(n_clusters=4, random_state=12345).fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three produced clusters closely match the topics World, Business, and Sci/Tech while the last and by far the largest cluster is less specific and contains words from all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (1703 instances)\n",
      "['.' '-' \"'\" 'iraq' 'president' 'sunday' 'would' 'security' 'government'\n",
      " 'people' 'afp' 'win' 'night' 'china' 'minister' 'bush' 'international'\n",
      " 'killed' 'city' 'stocks' 'european' 'talks' 'league' 'country' 'british'\n",
      " 'japan' 'india' 'police' 'prime' 'iraqi' 'leader' 'during' 'hit' 'say'\n",
      " 'baghdad' 'expected' 'election' 'her' 'north' 'war' 'australia'\n",
      " 'military' 'cut' 'nuclear' 'higher' 'un' 'official' 'palestinian' 'sox'\n",
      " 'attack' 'troops' 'russia' 'israeli' 'gaza' 'press' 'west' 'even'\n",
      " 'including' 'general' 'man' 'iran' 'football' 'forces' 'athens' 'past'\n",
      " 'europe' 'investors' 'peace' 'release' 'canadian' 'six' 'russian' 'beat'\n",
      " 'pakistan' 'public' 'eu' 'where' 'foreign' 'bomb' 'attacks' 'israel'\n",
      " 'nations' 'championship' 'korea' 'australian' 'kerry' 'leaders' 'french'\n",
      " 'men' 'death' 'killing' 'darfur' 'arafat' 'capital' 'army' 'japanese'\n",
      " 'campaign' 'race' 'france' 'vote']\n",
      "\n",
      "Cluster 1 (1713 instances)\n",
      "['us' 'company' 'oil' 'inc' 'york' 'yesterday' 'no' '?' 'corp' 'prices'\n",
      " 'years' 'group' 'season' 'deal' 'sales' 'business' 'billion' 'former'\n",
      " 'washington' 'profit' 'states' '/b&gt' 'b&gt' 'chief' 'american' 'shares'\n",
      " 'take' 'bank' 'third' 'federal' 'companies' 'co' 'maker' 'bid' 'plan'\n",
      " 'largest' 'big' 'giant' '5' 'growth' 'investor' '//www' 'href=http'\n",
      " '/a&gt' 'trade' 'earnings' 'dollar' 'buy' 'gold' 'union' 'amp' 'stock'\n",
      " 'loss' 'agreed' 'months' 'aspx' 'com/fullquote'\n",
      " 'target=/stocks/quickinfo/fullquote&gt' 'like' 'firm' 'air' 'rose'\n",
      " 'executive' 'jobs' 'update' 'boston' 'economy' 'drug' 'ahead' 'pay'\n",
      " 'near' 'biggest' 'economic' 'peoplesoft' 'car' 'o' 'street' 'work' 'your'\n",
      " 'free' '2005' '6' 'presidential' 'workers' 'wins' 'america' 'tokyo'\n",
      " 'nation' 'share' 'financial' 'fall' 'wall' 'house' 'fell' 'lower'\n",
      " 'september' 'crude' 'october' 'chicago' 'job']\n",
      "\n",
      "Cluster 2 (1801 instances)\n",
      "['' '(' 'by' 'reuters' 'ap' '&lt' 'u' 'microsoft' 't' 'game' 'percent'\n",
      " 'software' 'internet' 'market' 'announced' 'news' '2004' 'service'\n",
      " 'before' 'technology' 'com' 'search' 'computer' 'space' 'online' 'what'\n",
      " 'network' 'google' 'ibm' 'research' 'according' 'music' 'help' 'games'\n",
      " 'web' 'san' 'industry' 'mobile' 'services' 'quarter' 'wireless' 'system'\n",
      " 'data' 'i' 'phone' 'only' 'apple' 'oracle' 'windows' 'global' 'intel'\n",
      " 'found' 'users' 'reports' 'price' 'offer' 'use' 'uk' 'video' 'pc'\n",
      " 'systems' 'support' 'nasa' 'sun' 'contract' 'launch' 'linux' 'called'\n",
      " 'digital' 'scientists' 'net' 'used' '#36' 'program' 'version' 'future'\n",
      " 'center' 'site' 'products' 'customers' 'study' 'chip' 'sony' 'management'\n",
      " 'california' 'such' 'making' 'department' 'using' 'grand' 'ceo'\n",
      " 'university' 'tv' 'launched' 'times' 'source' 'server' 'too' 'better'\n",
      " 'phones']\n",
      "\n",
      "Cluster 3 (89485 instances)\n",
      "['the' ',' 'to' 'a' 'of' 'in' 'and' 's' 'on' 'for' '#39' ')' 'that' 'with'\n",
      " 'as' 'at' 'is' 'its' 'new' 'it' 'said' 'has' 'from' 'an' 'his' 'will'\n",
      " 'after' 'was' 'be' 'over' 'have' 'their' 'are' 'up' 'quot' 'but' 'more'\n",
      " 'first' 'two' 'he' 'world' 'this' '--' 'monday' 'wednesday' 'tuesday'\n",
      " 'out' 'thursday' 'one' 'not' 'against' 'friday' 'into' 'they' 'about'\n",
      " 'last' 'year' 'than' 'who' 'were' 'been' 'million' 'says' 'week' 'had'\n",
      " 'united' 'when' 'could' 'three' 'today' 'time' 'which' 'may' '1' 'off'\n",
      " 'team' 'next' 'back' 'saturday' 'or' '2' 'can' 'some' 'second' 'state'\n",
      " 'all' 'top' 'day' 'down' 'n' 'most' 'record' 'victory' 'officials'\n",
      " 'report' 'open' 'end' 'plans' 'court' 'if']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_array = model[0].to_numpy()\n",
    "for ci in range(kmeans.n_clusters):\n",
    "    cluster_words = np.compress(kmeans.labels_==ci, words_array)\n",
    "    print('Cluster {} ({} instances)'.format(ci, len(cluster_words)))\n",
    "    print(cluster_words[:100])\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
